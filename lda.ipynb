{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://kdd.ics.uci.edu/databases/20newsgroups/20_newsgroups.tar.gz -o 20_newsgroups.tar.gz\n",
    "\n",
    "#!tar -xvf 20_newsgroups.tar.gz.1\n",
    "\n",
    "#!pip install nltk\n",
    "\n",
    "#!pip install gensim\n",
    "\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, udf,rand\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import tokenize \n",
    "import nltk\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/21 14:17:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"lda\").master(\"local[*]\").config(\"spark.driver.maxResultSize\", \"4g\").getOrCreate()\n",
    "sc= spark.sparkContext\n",
    "wnl = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  id|                docs|\n",
      "+--------------------+--------------------+\n",
      "|comp.os.ms-window...|[comp, window, mi...|\n",
      "|  rec.sport.baseball|[sport, baseball,...|\n",
      "|comp.os.ms-window...|[comp, window, mi...|\n",
      "|           sci.crypt|[crypt, message, ...|\n",
      "|comp.os.ms-window...|[comp, window, mi...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir funciones de Transformacion\n",
    "\n",
    "def preprocess_file(file_path):\n",
    "    \"\"\"\n",
    "    Preprocesa un archivo de texto eliminando caracteres no alfanuméricos,\n",
    "    saltos de línea, espacios dobles y tabulaciones. Extrae el contenido\n",
    "    a partir de la línea siguiente a la primera ocurrencia de \"Lines:\" hasta\n",
    "    el final del archivo.\n",
    "    \"\"\"\n",
    "    with open(file_path, encoding=\"latin-1\") as f:\n",
    "        lines = f.readlines()\n",
    "        s = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"Lines:\" in line:\n",
    "                if s == 0:\n",
    "                    s = i\n",
    "        content = \"\".join([re.sub('[^a-zA-Z0-9.,\\s]+', \"\", x.replace(\"\\n\", \".\").replace(\"  \", \".\").replace(\"\\t\",\"\").replace(\"..\",\"\")) for x in lines[s + 4:]])\n",
    "        return content\n",
    "\n",
    "def feature_transformation(x):\n",
    "    \"\"\"\n",
    "    Realiza la transformación de características en un texto dado.\n",
    "    Tokeniza el texto, convierte las palabras a minúsculas, elimina las palabras\n",
    "    con una longitud menor o igual a 3, lematiza las palabras y elimina aquellas\n",
    "    que están en la lista de palabras vacías (Stopwords).\n",
    "    \"\"\"\n",
    "    words = list(tokenize(str(x)))\n",
    "    words = [word.lower() for word in words if len(word) > 3]\n",
    "    words = [wnl.lemmatize(word) for word in words if word not in Stopwords]\n",
    "    return words\n",
    "\n",
    "# Cargar Stopwords\n",
    "\n",
    "with open(\"Stopwords\") as file:\n",
    "    Stopwords = file.read().split(\"\\n\")\n",
    "\n",
    "\n",
    "directory_path = \"./20_newsgroups/\"\n",
    "#directory_path = \"./20_newsgroups/20_newsgroups\"\n",
    "folders = os.listdir(directory_path)\n",
    "\n",
    "dct_rdd = sc \\\n",
    "    .parallelize(folders) \\\n",
    "    .map(\n",
    "        lambda folder: (\n",
    "            folder, \n",
    "            [\n",
    "                preprocess_file(os.path.join(directory_path, folder, file)) for file in os.listdir(os.path.join(directory_path, folder))[:100]\n",
    "            ]\n",
    "        )\n",
    "    ) \\\n",
    "    .flatMap(lambda row: [(row[0], item) for item in row[1]]) \\\n",
    "    .map(lambda x : [x[0],x[1]])  \\\n",
    "    .map(lambda x : (x[0], feature_transformation(x)))\n",
    "\n",
    "df = dct_rdd.toDF([\"id\",\"docs\"])\n",
    "\n",
    "random_sample = df.orderBy(rand()).limit(5)\n",
    "random_sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorizacion del texto\n",
    "\n",
    "cv = CountVectorizer(inputCol = \"docs\", outputCol = \"features\")\n",
    "count_vectorizer_model = cv.fit(df)\n",
    "result = count_vectorizer_model.transform(df)\n",
    "corpus = result.select(\"id\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def python_func():\n",
    "    dct = {}\n",
    "    for folder in os.listdir(\"./20_newsgroups/20_newsgroups\"):\n",
    "        docs = []\n",
    "        for file in os.listdir(f\"./20_newsgroups/20_newsgroups/{folder}\")[0:100]:\n",
    "            with open(f\"./20_newsgroups/20_newsgroups/{folder}/{file}\", encoding = \"latin-1\") as f :\n",
    "                doc = f.readlines()\n",
    "                s = 0\n",
    "                for i,j in zip(doc,range(len(doc))):\n",
    "                    if \"Lines:\" in i:\n",
    "                        if s == 0:\n",
    "                            s= j\n",
    "                doc = \"\".join([re.sub('[^a-zA-Z0-9.,\\s]+', \"\", x.replace(\"\\n\", \".\").lower().replace(\"  \", \".\")) for x in doc[s+4:]])\n",
    "\n",
    "            docs.append(doc)\n",
    "        dct[folder] = docs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dividir en set the entrenamiento y de prueba\n",
    "corpus_train, corpus_test = corpus.randomSplit([0.7,0.3], seed=4000)\n",
    "\n",
    "# Entrenar modelo LDA\n",
    "lda_model = LDA(k= 10,seed=7,optimizer = \"em\", maxIter=10)\n",
    "fitted_model = lda_model.fit(corpus_train)\n",
    "topics = fitted_model.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/21 14:18:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/05/21 14:18:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/05/21 14:18:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/21 14:18:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+\n",
      "|            features|   topicDistribution|Topico Dominante|\n",
      "+--------------------+--------------------+----------------+\n",
      "|(41392,[0,1,2,3,4...|[0.11984254718708...|               7|\n",
      "+--------------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar el modelo ------ Encontrar el topico dominante \n",
    "\n",
    "fila_prueba = [corpus_test.select(\"features\").first()]\n",
    "document_topics = fitted_model.transform(spark.createDataFrame(fila_prueba))\n",
    "\n",
    "def extract_dominant_topic(topic_distribution):\n",
    "    return int(np.argmax(topic_distribution))\n",
    "extract_dominant_topic_udf = udf(extract_dominant_topic, IntegerType())\n",
    "\n",
    "df_with_dominant_topic = document_topics.withColumn(\n",
    "    \"Topico Dominante\",\n",
    "    extract_dominant_topic_udf(\"topicDistribution\")\n",
    ")\n",
    "\n",
    "\n",
    "df_with_dominant_topic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresion Logistica Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Corregir Df para que se ajuste al modelo\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"id\", outputCol=\"label\")\n",
    "indexer_model = indexer.fit(corpus_train)\n",
    "corpus_train_lr = indexer_model.transform(corpus_train)\n",
    "corpus_test_lr = indexer_model.transform(corpus_test)\n",
    "\n",
    "corpus_train_lr = corpus_train_lr.select(\n",
    "    [\n",
    "        \"label\",\n",
    "        \"features\"\n",
    "    ]\n",
    ")\n",
    "lr = LogisticRegression(\n",
    "    maxIter=4, \n",
    "    regParam=0.3, \n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "lr_multi_model = lr.fit(corpus_train_lr)\n",
    "transformed_lr_data = lr_multi_model.transform(corpus_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "20 X 41392 CSRMatrix\n",
      "\n",
      "Intercept: [0.11069798372035645,0.09797498783711699,0.0850879797438702,0.07203269374457213,0.07203269374459258,0.058804694513050794,0.03181191479130732,0.03181191479130724,0.031811914791311006,0.0040704242180268635,-0.010094242940290474,-0.010094242940310218,-0.0100942429403331,-0.024462321584411324,-0.039039710768368936,-0.05383256700729993,-0.08409068400014351,-0.08409068400012712,-0.09956968406646577,-0.18076882164776137]\n",
      "----------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 321:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.05571227080394922\n",
      "FPR: 0.05571227080394922\n",
      "TPR: 0.05571227080394922\n",
      "F-measure: 0.005880119430209736\n",
      "Precision: 0.003103857118132573\n",
      "Recall: 0.05571227080394922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \\n\" + str(lr_multi_model.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lr_multi_model.interceptVector))\n",
    "print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "trainingSummary = lr_multi_model.summary\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
